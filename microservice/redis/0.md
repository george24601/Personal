zset

Keyspace Notifications

ordered set: ziplist, skiplist, or balanced trees

### Zhihu's redis clusters

* 70TB total memory, used 40 TB
* 15M rps across the cluster
* single cluster 4M rps peak
* 800 clusters
* 20G storage or 200k rps - use cluster mode

Standalone: sentinel to do health check, raft to elect leaders, normally just 1 slave for standby, may add more read only slaves for r-w seperation 

Twemproxy by twitter

on restart, clear the cache to avoid dirty cache conflict with the old cache data


### official cluster
hash the key into 16384 Slots, user will assign slots to shard, on expanding, user will choose slots and MIGRATE keys in the slots. - All in sync, so will be in BLOCK mode -> cause p95 pikes or even failover for HUGE keys (> 1 MB) -

Client visiting migrating key will have to ASK twice.

These problems are motivation for non blocking MIGRATE

maxmemory

migration: SYNC/PSYNC, fork a process to iterate all RDB files and send to slave, all writes recieives will be buffered in memory, and will be sent to slave AFER all RDB files are transferred

1TB Resharding - 30 mins

Redis single thread means that MONITOR will make the CPU usage even higher under high CPU load

implemented by select, poll, epoll

redis snapshot BGSAVE and SAVE, AOF

This is the result of the fact that, without using pipelining, serving each command is very cheap from the point of view of accessing the data structures and producing the reply, but it is very costly from the point of view of doing the socket I/O. This involves calling the read() and write() syscall, that means going from user land to kernel land. The context switch is a huge speed penalty.

The reason is that processes in a system are not always running, actually it is the kernel scheduler that let the process run, so what happens is that, for instance, the benchmark is allowed to run, reads the reply from the Redis server (related to the last command executed), and writes a new command. The command is now in the loopback interface buffer, but in order to be read by the server, the kernel should schedule the server process (currently blocked in a system call) to run, and so forth. So in practical terms the loopback interface still involves network-alike latency, because of how the kernel scheduler works.

Basically a busy loop benchmark is the silliest thing that can be done when metering performances in a networked server. The wise thing is just avoiding benchmarking in this way.

redis network traffic volume needs to check - often a bottleneck

if a hot value is about to expire, can use mutex update or set another internal lease mechanism

eviction is done by timed + lazy, timed randonmly select keys every 100ms, so delete only when fetched

if many set the same key, then use distrubited lock or meta-version for CAS
