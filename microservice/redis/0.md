### Zhihu's redis clusters

* 70TB total memory, used 40 TB
* 15M rps across the cluster
* single cluster 4M rps peak
* 800 clusters
* 20G storage or 200k rps - use cluster mode

Standalone: sentinel to do health check, raft to elect leaders, normally just 1 slave for standby, may add more read only slaves for r-w seperation 

on restart, clear the cache to avoid dirty cache conflict with the old cache data


### official cluster
hash the key into 16384 Slots, user will assign slots to shard, on expanding, user will choose slots and MIGRATE keys in the slots. - All in sync, so will be in BLOCK mode -> cause p95 pikes or even failover for HUGE keys (> 1 MB) -

Client visiting migrating key will have to ASK twice.

These problems are motivation for non blocking MIGRATE

migration: SYNC/PSYNC, fork a process to iterate all RDB files and send to slave, all writes recieives will be buffered in memory, and will be sent to slave AFER all RDB files are transferred

1TB Resharding - 30 mins

This is the result of the fact that, without using pipelining, serving each command is very cheap from the point of view of accessing the data structures and producing the reply, but it is very costly from the point of view of doing the socket I/O. This involves calling the read() and write() syscall, that means going from user land to kernel land. The context switch is a huge speed penalty.

The reason is that processes in a system are not always running, actually it is the kernel scheduler that let the process run, so what happens is that, for instance, the benchmark is allowed to run, reads the reply from the Redis server (related to the last command executed), and writes a new command. The command is now in the loopback interface buffer, but in order to be read by the server, the kernel should schedule the server process (currently blocked in a system call) to run, and so forth. So in practical terms the loopback interface still involves network-alike latency, because of how the kernel scheduler works.

Basically a busy loop benchmark is the silliest thing that can be done when metering performances in a networked server. The wise thing is just avoiding benchmarking in this way.

eviction is done by timed + lazy, timed randonmly select keys every 100ms, so delete only when fetched

### tips

don't put code data into redis much - set proper expiry time

don't use redis for mq, can use disque

as per official doc: 50k conneciton + 50k qps at the same time

keep string size < 10kb, element size < 5000

delete non-string big key with hscan、sscan、zscan, instead of del

track # of evicted keys (probably done already)

SCAN in redis??

cache frag rate - jmalloc at 1.03, if < 1, means not enout memory
