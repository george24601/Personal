the Connector => generating the set of Tasks and indicating to the framework when they need to be updated.

A Kafka Connect cluster consists of a set of Worker processes that are containers that execute Connectors and Tasks

Kafka Connect does require persistent storage for offset data to ensure it can recover from faults, and users will need to configure this storage.

In distributed mode, you start many worker processes using the same group.id and they automatically coordinate to schedule execution of
connectors and tasks across all available workers. 

interaction with a distributed-mode cluster is via the REST API. To create a connector, you start the workers and then make a REST request
to create a connector 

Kafka Connect workers do not have a special “leader” process that you have to interact with to use the REST API; all nodes can respond to REST requests, including creating, listing, modifying, and destroying connectors.

How to install new plugins
--------------

Connector does not perform data copying. it is responsible for breaking that job into a set of tasks

For structured records, the Struct class should be used

Use SchemaBUilder to generate schema on the fly

record: k, v, partitionid, offset

connector implementation also responsible for montiroing external system

Developing connector: implement Connector and Task

------
example: org.apache.kafka.connect.file 
SourceConnector/SourceTask SinkConnector/SinkTask

getTaskConfigs => list of entries

SourceTask implementation
1.start() does not handle resuming
2.notice task's stop() is synchronized, because they run on a dedicatated thread, they will be stopped with a call from a different thread
in the Worker

poll() called repeatedly,may block, because KC gives each task a dedicated thread

commit() and commitRecord(), for ack mechanism for messages =>up to the offsets that have been returned by poll(). the implementaiton of
this API should block until the commit is complete.

commit handled by the framework, only the connector knows how to seek back to the right position in the input

SourceTask uses the SourceContext in its init() to seek the offset data

skipped sink task implementation
----

configuration validation??

to implement a source connector, need to decide when and how to create schemas
but dynamic shcemas are NORMAL espeically for DB connector

SchemaProjetors to handle compatible schemas



 
