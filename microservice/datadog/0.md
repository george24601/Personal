Tags are a way of adding dimensions to metrics

Tags are converted to lowercase.

For optimal functionality, we recommend constructing tags that use the key:value syntax.

device, host, and source are reserved tag keys and cannot be specified in the standard way

DogStatsD Tags

Tags in the API - note other endpoints support tags as well such as Events and Metrics

The recommended method is to rely on the integrations or via the configuration files. - The JMX integration collects metrics from applications that expose JMX metrics.

You can also enter tags: followed by a tag to see all the events that come from a host or integration with that tag. The example in the image is the tag role:cassandra. So the search text is tags:role:cassandra

A custom metric refers to a single, unique combination of a metric name, host, and any tags.

Custom metrics example
-----------

You submit the following metric name: auth.exceptionCount

Your code instrumentation plans the following tags associated with that metric: method:X, method:Y, exception:A, exception:B.

Note that the ordering of tags does not matter, so the following two metrics would be considered non-unique


```

Let’s say you want to have insight into the request.count from different services across your infrastructure.

You create your metric service.request.count
You want to separate the requests that were successful from the failures. You create two tags to that effect:

status:success
status:failure

You want this metric to be reported by each service running on your infrastructure. Let’s say you have 3 services per host:

service:database
service:api
service:webserver

```

The easiest way to get your custom application metrics into Datadog is to send them to DogStatsD, a metrics aggregation service bundled with the Datadog Agent.

As it receives data, DogStatsD aggregates multiple data points for each unique metric into a single data point over a period of time called the flush interval. 


1. edit your datadog.yaml file to uncomment the following lines

```
use_dogstatsd: yes
dogstatsd_port: 8125

```

you could compare the performance of two algorithms by tagging a timer metric with the algorithm version

```
@statsd.timed('algorithm.run_time', tags=['algorithm:one'])
def algorithm_one():
    # Do fancy things here ...

@statsd.timed('algorithm.run_time', tags=['algorithm:two'])
def algorithm_two():
    # Do fancy things (maybe faster?) here ...
```

The format for sending metrics is metric.name:value|type|@sample_rate|#tag1:value,tag2,


