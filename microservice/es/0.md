documents belongs to a type, and types live inside an index (just like a DB in RDBMS)

all data in every field is indexed by default

If you use it on a field containing an exact value, such as a number, a date, a Boolean, or a not_analyzed string field, then it will search for that exact value

As indices get even older, they reach a point where they are almost never accessed. We could delete them at this stage, but perhaps you want to keep them around just in case somebody asks for them in six months.

These indices can be closed. They will still exist in the cluster, but they won’t consume resources other than disk space. Reopening an index is much quicker than restoring it from backup.

With TTL , expired documents first have to be marked as deleted then later purged from the index when segments are merged. For append-only time-based data such as log events, it is much more efficient to use an index-per-day / week / month instead of TTLs. Old log data can be removed by simply deleting old indices.

Logging—and other time-based data streams such as social-network activity—are very different in nature. The number of documents in the index grows rapidly, often accelerating with time. Documents are almost never updated, and searches mostly target the most recent documents. As documents age, they lose value.

When you delete a document, it is only marked as deleted. It won’t be physically deleted until the segment containing it is merged away.

Aliases can help make switching indices more transparent. For indexing, you can point logs_current to the index currently accepting new log events, and for searching, update last_3_months to point to all indices for the previous three months:

To achieve high ingest rates, you want to spread the shards from your active index over as many nodes as possible.
For optimal search and low resource usage you want as few shards as possible, but not shards that are so big that they become unwieldy.
An index per day makes it easy to expire old data, but how many shards do you need for one day?
Is every day the same, or do you end up with too many shards one day and not enough the next?

Rollover Pattern
--------
There is one alias which is used for indexing and which points to the active index.
Another alias points to active and inactive indices, and is used for searching.
The active index can have as many shards as you have hot nodes, to take advantage of the indexing resources of all your expensive hardware.
When the active index is too full or too old, it is rolled over: a new index is created and the indexing alias switches atomically from the old index to the new.
The old index is moved to a cold node and is shrunk down to one shard, which can also be force-merged and compressed.

The rollover API would be called regularly by a cron job to check whether the max_docs or max_age constraints have been breached. As soon as at least one constraint has been breached, the index is rolled over. Since we’ve only indexed 5 documents in the example above, we’ll specify a max_docs value of 5, and (for completeness), a max_age of one week:

