It’s choosing a few key metrics to keep an eye on, spend less time tracking, and more time acting upon the found data.

Monthly recurring revenue (MRR)
These metrics measure a product’s total revenue in one month

Average revenue per user (ARPU) allows you to count the revenue generated per user monthly or annually.

There are two types of ARPU: per new account and per existing account. ARPU per new account refers to metrics based on new accounts appearing after the subscription plan or product price was changed. ARPU per existing account involves the data from accounts established before the price change.

Average Revenue Per User (ARPU) * Average customer lifetime = CLTV

Sales & marketing spendings for a period of time / total # of customers generated for a period of time = CAC

Daily Active User/Monthly Active User ratio
DAU/MAU of 20 percent is considered a good sign, while 50+ percent indicates extreme success

Customer retention rate (CRR) is the percentage of customers who stayed with the company after a certain time period

According to the Product Benchmarks Report by Mixpanel, the average CRR for most software products is below 20 percent over 8 weeks

It can be tracked with statistics that display the number of logins or site visits. This KPI reveals the popularity of a product – if the audience engages with it again and again. Unlike traffic or session duration, the number of sessions per user shows an average for a particular group of people in some time period.

This KPI seems similar to the previous one, but it tracks not just how many times a user opened an app. It displays which actions a user made and which feature(s) they used while using the app.

The core principles you should focus on as a product manager revolves around product usage, customer-centric design (especially user experience), competitive offerings, pricing, market share and industry trends.

Customer Acquisition Cost (CAC)
Customer Conversion Rate (CCR)
Repurchase Rate (RR)
Daily Active Users (DAU)
Feature Usage (FU, yes that’s really the abbreviation)
User Churn (UC)
Net Promoter Score (NPS)
Customer Satisfaction (CSAT)
Customer Lifetime Value (CLV)

, the number one priority for your data should be finding the overall “product-market fit.” Usage data (like events) is a good place to start for this, but you absolutely need to include customer feedback. One manager I talked to said that no observational usage data has ever stacked up to the insight she gains from picking up the phone and talking to 10 customers directly. As a general rule I agree with her but it’s not just because conversations have magical power (though they do) it’s because you probably haven’t setup formal tests for evaluating the usage data.

This should go without saying, but talk to your customers prior to building anything.  People love talking about their problems to anyone who will listen to them.  Often they won’t have the first clue about what a solution looks like, but at the very least repeated similar emotional reactions from many people in a market should tell you that the problem is there and real.  After that, it is “just” a matter of marketing.

 That is 100% avoidable if you simply don’t commit to schedules.  (Also note that committing to a schedule is time debt, by definition.  If you ever say “Yes, I will implement that”, you’ve lost the ability to decide not to implement it if your priorities change.)

 “The only acceptable response to a feature request is: ‘Thank you for your feedback.  I will take it under advisement and consider it for inclusion in a later version of the software.’”  That line actually works.  (There are industries and relationships in which it won’t work — for example, if you’re in a regulated industry and the regulations change, you can’t fob the regulatory authority off with that.  Don’t be in a regulated industry.)


### User stories applied

tech vs customer/analyst/domain expoert communication: either side dominates => project loses

Story: 1. description for planning and reminder, functionality valued by USER
  2. conversations about the story as details
  3. tests that can be used to determin when a story is complete
  4. needs to be done by CUSTOMER, and NOT in tech jargons

When a story is too large it becomes epics

But we do not break down story into every last detail. Talk with customer about details when details become important. 

But story is not contract, test is!

customer team: tests, PM, real users (as comprehensive as possible), interaction designers
They are visioneris, and write story in a story writing workshop 

velocity: how much work a developer can do in an iteration

customer team can make corrections before the start of each iteration

identify user roels and personas before writing stories

need reasonable representation of the devs and customers in the init BS session, where each one comes up with new roles as much as you can,
and categorize them => at least cover MVP case

also need to rip unimportant role cards for roles that are unimportant to the success of the system

minimal metrics for user:
1. use frequency
2. expertise with the domain
3. level of proficiency with the software being developed
4. general goal for using the software

persona: make sure enough market and demo research done

extreme character: unlikely users for the systems, to lead to stories that you may miss
try not to include dependency between stories

When as much detail is added , the dicussions abou the story ar emuch morelikely to feel concrete and real. This can lead to the mistaken belief that the story reflects all details

details that been determined through conversations become tests

many stories are evaluated by purchasers than users

variate tech-dev focused term into true user-story,i.e., user-value

if dev does not have the domain knowledge, talk to the user to figure out

Bug reports and user interface changes often are too small for a story => combine them to a bigger one

responsible for helping the customer write stories that are promises to converse rather than detailed specifications

There is one more thing worth mentioning about breaking up user stories that many teams get wrong. When slicing stories, they should be sliced vertically, rather than horizontally.

Formal structures like tech leads can serve to make this even worse if the domains are particularly small. I tend to prefer establishing DRIs for sub-priorities with a team. The DRI is responsible for a sub-priority or sub-component but it's not their identity; they're still expected to work on other stuff on the team and to focus their energies on whatever matters most to the team as a whole. (Important to set expectations)

By creating vertically-sliced stories, the team has a clear picture of what specific tasks need to be completed to add value for the end-user, and the tasks are more discrete and easier to keep track of.

If team members are feeling overwhelmed then you've gone too far. Don't throw your intern to the sharks
