 a difference in significance does not always make a significant difference.

Statistical power => higher the power, less likely to make a false negative => i.e.,more likely to detect it when the effect is actually
there

Instead of independently comparing each drug to the placebo, we should compare them against each other. We can test the hypothesis that they
are equally effective, or we can construct a confidence interval for the extra benefit of Fixitol over Solvix. If the interval includes
zero, then they could be equally effective; if it doesn’t, then one medication is a clear winner. This doesn’t improve our statistical
power, but it does prevent the false conclusion that the drugs are different. Our tendency to look for a difference in significance should
be replaced by a check for the significance of the difference

tandard error tells me how a statistic, like a mean or the slope of a best-fit line, would likely vary if I take many samples of patients. A
confidence interval is similar, with an additional guarantee that 95% of 95% confidence intervals should include the “true” value

---------------
