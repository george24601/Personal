If we know the dist of cold cases, we can tell how likely it is for a random sample of cold patients to have cold length in relation to
average => if my medication is completely ineffective, what are the chances I'd see data like what I saw?

p value => under the assumption of no effect or no difference, of obtaining a result equal to more extreme that what is observed

A bigger difference, or one backed up by more data, suggests more surprise and a smaller p value.

A statistically insignificant difference does not mean there is no difference at all.

and if the data is sparse or unclear, your conclusions are uncertain.

----------------

 we might miss a viable medicine or fail to notice an important side-effect. How do we know how much data to collect?

Let’s say we look for a p value of 0.05 or less, as scientists typically do. That is, if I count up the number of heads after 10 or 100
trials and find a deviation from what I’d expect – half heads, half tails – I call the coin unfair if there’s only a 5% chance of getting a
deviation that size or larger with a fair coin

You can see that if the coin is rigged to give heads 60% of the time, and I flip the coin 10 times, I only have a 20% chance of concluding
that it’s rigge

Scientists are usually satisfied when the statistical power is 0.8 or higher, corresponding to an 80% chance of concluding there’s a real
effect.

 many trials conclude with “There was no statistically significant difference in adverse effects between groups” without noting that there
was insufficient data to detect any but the largest differences

------------
 Had I collected data from 1,000 independent patients instead of repeatedly testing 100, I would be more confident that differences between
groups came from the medicines and not from genetics and luck. I claimed a large sample size, giving me statistically significant results
and high statistical power, but my claim is unjustified.


eudoreplication occurs when individual observations are heavily dependent on each other. To defend against it:
1. Average the dependent data points.
2. Analyze each dependent data point separately. => multiple comparison problem
3. Use a statistical model which accounts for the dependence, like a hierarchical model or random effects model.

-------------
We’ll assume my tests have a statistical power of 0.8. Of the ten good drugs, I will correctly detect around eight of them, shown in purple

 P values are calculated under the assumption of no effect, so p=0.05p=0.05 means a 5% chance of falsely concluding that an ineffectual drug
works. => The chance of any given “working” drug being truly effectual is only 62%

e discovery rate – the fraction of statistically significant results which are really false positives – is 38%. => base rate fallacy!!!

--------------

Despite the test only giving false positives for 7% of cancer-free women, analogous to testing for p<0.07p<0.07, 91% of positive tests are
false positives.

likelihood of a positive result being true depends on what proportion of hypotheses tested are true => base rate fallacy again!

urvey overestimate the use of guns in self-defense? Well, for the same reason that mammograms overestimate the incidence of breast cancer:
there are far more opportunities for false positives than false negatives.

To lower p, criminologists make use of more detailed surveys

Making multiple comparisons means multiple chances for a false positive.

Correct for multiple comparison =>  For example, the Bonferroni correction method, but
lowering the p threshold required to declare a result statistically significant, you decrease your statistical power greatly, and fail to
detect true effects as well as false ones

A scientist is more interested in the false discovery rate: what fraction of my statistically significant results are false positives? Is
there a statistical test that will let me control this fraction?

=> Benjamini-Hochberg procedure
