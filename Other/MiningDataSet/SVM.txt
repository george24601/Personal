Instance space x: binary or real-valued feature vector x of word occurences, d features
, class y

Binary classification: input vectors x(j) and lablesl y(j), find vector w s.t. f(x) = 1 if w * x >= theta
-1 otherwise

linear classifier: each feature has a weight, prediciton is based on w * x => positive 1, negative -1

-----------

separate + from - using a line:
training examples => each example with x(i), and y(i)

which is the best linear separator?
Margin r: distance of cloest example from the decision line/hyperplace

Why maximizing it a good idea?

Consider point A and line L: d(A, L) = w * A + b
L: w * x + b = 0

so confidence = (w* x + b) y, because we predict on the sign of w*A + b
to get larget margin, we want to solve
max of (min margin distance over all points i) over all w

-----------
separating hyperplane is defined by the support vectors
points on +/- planes from the solution
if no degeneracies => d+1 support vector for d dim data

but scaling w increases margin => work with normalized w. also require support vector to be on the place defined by w * x + b  is 1 or -1

want to maximize r => r = 1 /|w| => so we can rewrite SVM as 
min 1/2 ||w||^2 s.t. for all i, y(i)*(w * x(i) + b) >= 1 over all w

-------
if data is not separable introduce penalty C to our SVM
use slack variables/penalty, since not all mistakes are equally bad => if x(i) is on the wrong side of the margin then get penalty
and y * (w * x + b) >= 1 - penalty

SVM natural form: add regularization param C * "hinge loss"

-----------
Want to estimate w, b => instead of natural form, minimize f(w,b)
SVM solver inefficient for big data
so gradient descent on f but gradiant of delta(j) take s O(n) time =>i.e. need to check all data in the training set

Stochastic gradient descent
instead of evaluating gradient over all examples, eval it for each individual training example

---------
