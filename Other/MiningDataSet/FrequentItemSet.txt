Support for itemset I = # of baskets containing all items in I
support threshold s: sets of itmes that appear in at least s baskets: frequent itemsets

Example:
Basket= sentences, items=documents containing those sneences
items do not have to be "in" baskets, i.e., just need many to many relationship

Baskets=documents; items=words

Confidence of Assocuation rules: 
{i1,i2,...ik}-> j: if all i1,i2,...,ik are in the set, how likely j is in the set as well?

Algorithm:
Find all sets with support at least cs
Find all sets with suport at least s
from cs, remove one element, see which subsets have support at lease s.
How to compute confidence in this algo?

Computation model:
data kept basket-by-bastket on flat file

=> hardest problem is finding the frequent pairs. Support thresold is et high enought that you dont get too many frequesnt itemsets
-------
monotonicity: if a set of items appears at least s times, so does eveyr subset of S
Conversely, if item i does not appear in s, then no pair including i can appear in s

A-Priori:
Read baskets and count in main memory the occurence of each time
Items that appear at least s times are the frequent items
Read bakets again and count in main memory only those pairs both of which were found in pass 1 to be frequent

Can use the triangular matrix method with n = # of frequent items

So use more passes to extend to k-sets from 2
C1 -> filter -> L1 -> construct->c2 -> filter -> L2
---------
Improvement (PCY):
in pass 1, keep counts of buckets into which pairs of items are hashed
for each basket, enumerate all its pairs, hash them, and increment the resulting bucket count by 1

a bucket is frequent if its count is at least the support threshold. We can use rest of memory as buckets

Threadsold for PCY to beat a priori

Multistage: after pass 1 of PCY, rehash pairs that qualitfy for pass 2 of PCY
Multihash: use serveral independent hash tables on the first pass

------------
All frequent itemsets

Sampling approach: 

SON algorithm: repatedly read small subsets of the baskets into main memory and perform the first pass of the simple algorithm on each subset
Candidate if frequent in any subset run

On a second pass, count all the candidate itemsets and determine which are frequent in the entire set

We can distribute local frequent itemsets, then distribute the candidates from each node
Each node counts all the candidate itemsets
Accumulate the counts of all candidates

Toivonen's algorithm
Add to the itemsets that are frequent in the sample and the negative border of these itemsets
An itemset is in the negative border if it is not frequent in the sample, but all its immediate subsets are
In a second pass, count all candidate frequent itemsets from the first pass, and also count their negative border

Theorem and proof:
If there is an itemset that is frequent in the whole, but not frequent in the sample, then there is a member in the negative border that is frequent in the whole










