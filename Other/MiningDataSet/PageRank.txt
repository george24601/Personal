Incoming link as votes => links from important pages are more important

Each link's vote is porportional to the importance of its course page => split among importance/n votes => the page itself is defined this way as well

so force sum of all ranks = 1 to solve the linear system

-----
Stochastic adjacency matrix M: columns sum to 1

Rank vector r: so r = M * r,i.e., r is an eigenvector with eigenvalue 1

How to solve eigenvector? power iteration

-------
Power iteration:
Init r0 = 1/N * 1
Keep interating until
r(i+ 1) = M * r(i) is within epsilon. If not converged, go to 1

Random walk interpretation: p(t)= vector whose ith coordinate is page i at time t
So relationship between p(t+1) and p(t)
=>
Stationary distribution of random walk, for certain conditions

stationary distribution is unique and eventually will be reached no matter
waht the intial probablily distribution at time t = 0
------

Converge: 
Spider trap (out links within the group): trap will absorb all importance
dead end: importance will leak out

---
solution: 
With prob P, follow link at random
with probm 1-p, teleport to a random page
between 0.8-0.9

So surfer will jump out of spider trap with in a few time steps

For dead one, teleport to a random page=> how to translate this into M?

-----
Markov chains in PR: transition matrix P where Pij = P(Xt=i | xt-i = j)

For any start vector, power method will converge to a unique postive
stationary vector as long as P is stochastic, irreducible, and aperiodic

Stochastic: every col sums to 1. so give dead end page special link to all
pages
periodic: exists k > 1 s.t. interval between two visits to some state s is
always a multiple of k. A simple cycle exists. So add special link to itself. 
Irreducible: always can go to one state to another state

final forumla: notice the 1-p part
rj =?
So transform M to A to take random jump into account
A = P*M +(1-P)/n * e * eT, So that r(t+1) = A * r(t)

-------
IRL, how to compute r(new) = A * r(old) => we can handle only the sparse matrix. so cant compute with A directly
for leaked score, evenly redistribute among rs (i.e., jumping)

-------------

final form of page rank

M is sparse, but most likely wont fit in memory, but will fit in disk
In power-iteration, each iteration we read r(old) and M, and write r(new) to disk
What if r(new) is too big for memory? 
block-based update
break r(new) into k blocks, scan M and r(old) once for each block

improvement: block-stripe update
break M into stripes
Each strip contains only destination nodes 

problems with pageRank:
biased toward topic-specific authorities
other models
link spam
-----
topic specific: when walker teleports, she pick a page from a set S => only
pages that are relevant to the topic => i.e., and r(s) for teleport set S

--------

SimRank: random walks from a fixed node on k-partite graphs => random walk with restarts 
bu doesnt not scale well

-----
spam: topology => get y's page rank in related to M/N

----
TrustRank: topic-specific page rank with teleport set of trusted pages
filtering: thresold/spam mass


