#!/bin/bash
<<TOP
* Note that `top` has different layouts on centos and os x
* Are there too many users? Load average should be < 0.5 * # of cores
* If `us` is too high, then user processes are consuming much CPU resources, use `top -p $PID`
* If `sy` is too high, what is kernel working on?
* If `wa` is too high, the cpu idle waits for IO too much.
* If `Swap` > 0, are we running out of physical RAM?
* Check if `VIRT` is within your expectation - You should have a basic idea on how much virtual memory a process uses.

ni: high nice value user space process

Tasks
-------
PR: it is the process actual priority, as viewed by the Linux kernel, the scheduling priority of the process. For normal processes, the kernel priority is simply +20 from the nice value. Thus a process with the neutral nice value of zero has a kernel priority of 20. For processes running under real-time, the value of the field is RT

RES: It is the resident memory size. Resident memory is the amount of non-swapped physical memory a task is using. It stands for the resident size, which is an accurate representation of how much actual physical memory a process is consuming. (This also corresponds directly to the %MEM column.)

SHR: SHR is the shared memory used by the process and it indicates how much of the VIRT size is actually sharable (memory or libraries). In the case of libraries, it does not necessarily mean that the entire library is resident.

%MEM: It represents the percentage of available physical memory used by the process.

TIME+: The total CPU time the task has used since it started, with precision up to hundredth of a second.

COMMAND: The command which was used to start the process

cache is page cache/disk cache in memory
cache for file system I/O, also used as other deviced's cache, e..g, block device
buffer cache is for block device IO -, also memory managed by block

number of online customers is more straightforward, number of concurent request is more accurate
TOP


#try to limit the average load to < 0.7 * CPU cores 
top -p pid_value

top -U franshesco

#sort by %CPU
top -o %CPU

<<CS
note may need to install sysstat
check # of CSs, possible caues
1. normal CS by CPU
2. has higher priority tasks
3. I/O blocking
4. resourse contention and failed to acquire
CS
#check context switch times, 10 times, once per second, see if cs >> in. Can use pidstat on top of it
# if si(swap into memory) and so(swap out to harddrive) often > 0, means memory may not be enough
# us(user execution time) too high, means user process consumes too much CPU 
# wa(IO wait percentable), means IO wait too much
vmstat -1 -10

#check memory, unit kb, buffer is buffer cache in memory, 
#second line is the memory used by the program = free + buffers + cached. Note that buffers and cached are considered used from OS pov, note the difference between buffer and cache
#grep oom_killer is /var/log/messages
free -h


#It is best to run iostat specifying a time interval in seconds (for example iostat -x 30)

#Display a continuous device report at two second intervals

<<IOSTAT
rrqm/s: The number of read requests merged per second that were queued to the device

rsec/s: The number of sectors read from the device per second.

avgrq-sz:The average size (in sectors) of the requests that were issued to the device.

await:The average time (in milliseconds) for I/O requests issued to the device to be served. This includes the time spent by the requests in queue and the time spent servicing them.

if await > svctm, often a sign of long IO queue

iowait in top: wait because of IO, CPU is idle at this time, not a strong indicator itself

%util
Percentage of CPU time during which I/O requests were issued to the device (bandwidth utilization for the device). Device saturation occurs when this value is close to 100%.

%idle < 70% often a sign of busy IO

IOSTAT
iostat x
iostat -dc 10

###jstack

#shows the heapsize
java -XX:+PrintFlagsFinal -version | grep HeapSize

#find jvm thread with highest cpu usage, can use jps to get java process id
#you can also use jps if you know the process name
top -H -p 63847

#find thread 2803, 2833 high cpu usage within that thread

# find java processes
ps axu | grep java

#thread dump
sudo jstack 63847 > 1.log

#need to covert decimal pid to hex
echo "obase=16; 3747" | bc

#if you need one-off stack, kill -3 would show the dump as well
kill  -3 63847

### info on GC 
jstat -gcxx -t pid $INTERVAL $COUNT

#check heap usage 
jmap -heap -dump:file=xxx $PID 
#check object number and size
jmap -histo:live pid $PID 

#show java process id or can just ps aux | grep java. Here you can see the importance to giving meaninigful thread name
jps -v 

#view the port associated with a daemon
lsof -i -n -P | grep sendmail

#list open files in the open space
lsof -P -n | wc -l

#show deleted but still referenced files, so use 
lsof | grep deleted
#don't just rm! echo write instead. Otherwise, df still shows referenced file size
echo "" > tomcat.log

###last time  system online
uptime

###who are on this machine
who -a

###uname -a
system version

#under current dir, each dir's space
du -h --max-depth=1

#free space at each mouting point
df -h

#check a process's memory usage 
pmap $PID

# check process parent-child relationship
pstree 

### Note that 32G 128G physical machine can be considered high end already, should be able to handle 100k+ rqs

<<GC

reduce GC frequence to increasing heap size
reduce GC STW, by reducing heap space or use CMS
avoid Full gc by changing CMS trigger ration, give old gen more space, and incrase GC threads number
GC

