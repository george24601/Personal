#  Data structure

We abstract Pump into a simple KV database and store in leveldb. The Key is start_ts for Prewrite binlog or commit_ts or Commit binlog, while the Value is the binlog metadata. Binlog data stays in the data file

P appends the binlog to file, and ts, type, len, file, file offset to level db


# Pump

Assume we don't ignore error

if failed to write p-binlog: txn aborts
if writingn to p-binlog is good, but prewrite failed
or failed to write c-binlog: pump will wait for max txn timeout (default 10 mins), to ask tikv for that txn's status, - this means the sync delay

pump uses a kv to store binlog 

If the transaction fails, it will write a Rollback binlog record.

Pump records the binlog generated by TiDB in real time, and sorts the binlog according to the commit time of the transaction

* pump by default keeps 7 days 
* how to check the replication lag at pump and drainer?
* make sure keep 1 pump on at all times. 

or the Commit binlog, Pump Client will send it to the Pump instance selected by the corresponding Prewrite binlog.

When Drainer requests the binlog after a specified timestamp from Pump, Pump queries the metadata of the corresponding binlog in LevelDB. If the current data is the Prewrite binlog, Pump needs to find the corresponding Commit binlog before going ahead; if the current data is the Commit binlog, Pump continues to move forward.

 To avoid this issue, Pump3 needs to notify Drainer that it goes online. After receiving the notification, Drainer will add Pump3 to the merging and sorting list and returns a successful feedback to Pump3, and only then can Pump3 provide the writing binlog service.


# Drainer
```
drainer.toml:
log-file="drainer.log"
[syncer]
db-type="mysql"
[syncer.to]
host="127.0.0.1"
user="root"
password=""
port=3306
```
```
tidb.toml:
store="tikv"
path="127.0.0.1:2379"
[log.file]
filename="tidb.log"
[binlog]
enable=true
```

Here we can already see the tidb_binlog database, which contains the checkpoint table used by drainer to record up to what point binary logs from the TiDB cluster have been applied.

If you kill a Drainer, the cluster puts it in the “paused” state, which means that the cluster expects it to rejoin,i.e., it is different from the "offline" mode! Use binglogctl to clean up the metadata

The main difference is that since TiDB is a distributed database, the binlog generated by each TiDB instance needs to be merged and sorted according to the time of the transaction commit before being consumed downstream.

Drainer collects and merges the binlog from each Pump instance, and then converts the binlog into SQL statements or data in the specified format, and finally pushes the data to the downstream.

Before replicating the binlog data to the downstream, Drainer needs to convert the above data back into SQL statements.

