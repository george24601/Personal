
Doc values are the on-disk data structure, built at document index time, which makes this data access pattern possible. They store the same values as the `_source` but in a column-oriented fashion that is way more efficient for sorting and aggregations

The value of the `_id` field is also accessible in aggregations or for sorting, but doing so is discouraged as it requires to load a lot of data in memory. In case sorting or aggregating on the `_id` field is required, it is advised to duplicate the content of the `_id` field in another field that has doc_values enabled.

`_doc` has no real use-case besides being the most efficient sort order. So if you don’t care about the order in which documents are returned, then you should sort by `_doc`.

Nested document don’t have a `_source` field, because the entire source of document is stored with the root document under its `_source` field. To include the source of just the nested document, the source of the root document is parsed and just the relevant bit for the nested document is included as source in the inner hit. Doing this for each matching nested document has an impact on the time it takes to execute the entire search request, especially when size and the inner hits' size are set higher than the default. To avoid the relatively expensive source extraction for nested inner hits, one can disable including the source and solely rely on doc values fields. 

Elasticsearch has no concept of inner objects. Therefore, it flattens object hierarchies into a simple list of field names and values.

The`_nested` metadata is crucial in the above example, because it defines from what inner nested object this inner hit came from. The field defines the object array field the nested hit is from and the offset relative to its location in the `_source`

By default the `_source` is returned also for the hit objects in inner_hits, but this can be changed. Either via `_source` filtering feature part of the source can be returned or be disabled. If stored fields are defined on the nested level these can also be returned via the fields feature.

The user.first and user.last fields are flattened into multi-value fields, and the association between alice and white is lost. This document would incorrectly match a query for alice AND smith

nested objects index each object in the array as a separate hidden document, meaning that each nested object can be queried independently of the others with the nested query

geoPoint - can not be auto-converted, have to specify data type

Query types: 
* geo_bounding_box: bound by tl and br points, in memory filtering. Possible to speed up with II
* geo_distance: use the circle's square to filter out possible ones, and then check each point's dist to the center. Defaults to ark. Can switch to plane mode to speed up. Can use II's range query
* geo_polygon: slow.

Aggregation: 
* geo_distance
* geohash_grid
* geo_bounds

### Pagination

scroll with search context - seems not many use cases
Search After API, the aggregation must be uniquely sorted. Good for single direction deep pagination

* use `_source` to include only required info
* match, match_phrase: full text search on text type
* term, terms, range: use the II directly

use boost to change query weight

bool query: must, should, must_not, and filter


### Handling slow geo query:
add filter early, before sort

```json
 filter: {
	  must: [
      { term: { category: "chinese" }},
      { term: { city: "san francisco" }}
    ]
  }
```



# Shard size & number selection 
By default, each index in Elasticsearch is allocated 5 primary shards and 1 replica which means that if you have at least two nodes in your cluster, your index will have 5 primary shards and another 5 replica shards (1 complete replica) for a total of 10 shards per index.

As data is written to a shard, it is periodically published into new immutable Lucene segments on disk, and it is at this time it becomes available for querying. This is referred to as a refresh.

As the number of segments grow, these are periodically consolidated into larger segments. This process is referred to as merging. As all segments are immutable, this means that the disk space used will typically fluctuate during indexing, as new, merged segments need to be created before the ones they replace can be deleted. Merging can be quite resource intensive, especially with respect to disk I/O.

There is no fixed limit on how large shards can be, but a shard size of 50GB is often quoted as a limit that has been seen to work for a variety of use-cases.

As segments are immutable, updating a document requires Elasticsearch to first find the existing document, then mark it as deleted and add the updated version. Deleting a document also requires the document to be found and marked as deleted.

all data in every field is indexed by default

If you use it on a field containing an exact value, such as a number, a date, a Boolean, or a not_analyzed string field, then it will search for that exact value

For each Elasticsearch index, information about mappings and state is stored in the cluster state. This is kept in memory for fast access. Having a large number of indices in a cluster can therefore result in a large cluster state, especially if mappings are large. This can become slow to update as all updates need to be done through a single thread in order to guarantee consistency before the changes are distributed across the cluster.

Small shards result in small segments, which increases overhead. Aim to keep the average shard size between a few GB and a few tens of GB. For use-cases with time-based data, it is common to see shards between 20GB and 40GB in size.

The number of shards you can hold on a node will be proportional to the amount of heap you have available, but there is no fixed limit enforced by Elasticsearch. A good rule-of-thumb is to ensure you keep the number of shards per node below 20 to 25 per GB heap it has configured. A node with a 30GB heap should therefore have a maximum of 600-750 shards, but the further below this limit you can keep it the better. This will generally help the cluster stay in good health. shards result in small segments, which increases overhead. Aim to keep the average shard size between a few GB and a few tens of GB. For use-cases with time-based data, it is common to see shards between 20GB and 40GB in size.

# Rollover Pattern
As indices get even older, they reach a point where they are almost never accessed. We could delete them at this stage, but perhaps you want to keep them around just in case somebody asks for them in six months.

These indices can be closed. They will still exist in the cluster, but they won’t consume resources other than disk space. Reopening an index is much quicker than restoring it from backup.

With TTL , expired documents first have to be marked as deleted then later purged from the index when segments are merged. For append-only time-based data such as log events, it is much more efficient to use an index-per-day / week / month instead of TTLs. Old log data can be removed by simply deleting old indices.

Logging—and other time-based data streams such as social-network activity—are very different in nature. The number of documents in the index grows rapidly, often accelerating with time. Documents are almost never updated, and searches mostly target the most recent documents. As documents age, they lose value.

When you delete a document, it is only marked as deleted. It won’t be physically deleted until the segment containing it is merged away.

Aliases can help make switching indices more transparent. For indexing, you can point logs_current to the index currently accepting new log events, and for searching, update last_3_months to point to all indices for the previous three months:

To achieve high ingest rates, you want to spread the shards from your active index over as many nodes as possible.
For optimal search and low resource usage you want as few shards as possible, but not shards that are so big that they become unwieldy.
An index per day makes it easy to expire old data, but how many shards do you need for one day?
Is every day the same, or do you end up with too many shards one day and not enough the next?

There is one alias which is used for indexing and which points to the active index.
Another alias points to active and inactive indices, and is used for searching.
The active index can have as many shards as you have hot nodes, to take advantage of the indexing resources of all your expensive hardware.
When the active index is too full or too old, it is rolled over: a new index is created and the indexing alias switches atomically from the old index to the new.
The old index is moved to a cold node and is shrunk down to one shard, which can also be force-merged and compressed.

The rollover API would be called regularly by a cron job to check whether the max_docs or max_age constraints have been breached. As soon as at least one constraint has been breached, the index is rolled over. Since we’ve only indexed 5 documents in the example above, we’ll specify a max_docs value of 5, and (for completeness), a max_age of one week:

