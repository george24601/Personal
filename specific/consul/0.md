Seems that consul agent health check every 10 secs - spring.cloud.consul.discovery.health-check-interval

Consul treats the state of the agent as authoritative; if there are any differences between the agent and catalog view, the agent-local view will always be used.

In addition to running when changes to the agent occur, anti-entropy is also a long-running process which periodically wakes up to sync service and check status to the catalog. < 128 node cluster - 1 min syncs


if building a new server with the same IP isn't an option, you need to remove the failed server. Usually, you can issue a consul force-leave command to remove the failed server if it's still a member of the cluster.

As a sanity check, the consul info command is a useful tool. It can be used to verify raft.num_peers is now 2, and you can view the latest log index under raft.last_log_index. When running consul info on the followers, you should see raft.last_log_index converge to the same value once the leader begins replication. That value represents the last log entry that has been stored on disk.

Each node in a cluster must have a unique name. By default, Consul uses the hostname of the machine


----------
We don't cover Consul's multi-datacenter capability here, but as long as --net=host is used, there should be no special considerations for Docker.

The cluster address is the address at which other Consul agents may contact a given agent. The client address is the address where other processes on the host contact Consul in order to make HTTP or DNS requests. You will typically need to tell Consul what its cluster address is when starting so that it binds to the correct interface and advertises a workable interface to the rest of the Consul agents.

Note that the agent defaults to binding its client interfaces to 127.0.0.1, which is the host's loopback interface. This would be a good configuration to use if other containers on the host also use --net=host, and it also exposes the agent to processes running directly on the host outside a container, such as HashiCorp's Nomad.

If you want to expose the Consul interfaces to other containers via a different network, such as the bridge network, use the -client option for Consul

--------
One option is to use a custom DNS resolver library and point it at Consul. 
Another option is to set Consul as the DNS server for a node and provide a recursors configuration so that non-Consul queries can also be resolved. 
The last method is to forward all queries for the "consul." domain to a Consul agent from the existing DNS server.

In DNS, all queries are case-insensitive

The format of a standard service lookup is:
```
[tag.]<service>.service[.datacenter].<domain>
```

by default, Consul does not resolve DNS records outside the .consul. zone unless the recursors configuration option has been set. As an example of how this changes Consul's behavior, suppose a Consul DNS reply includes a CNAME record pointing outside the .consul TLD. The DNS reply will only include CNAME records by default. By contrast, when recursors is set and the upstream resolver is functioning correctly, Consul will try to resolve CNAMEs and include any records (e.g. A, AAAA, PTR) for them in its DNS reply.


-----------

Every token has an ID, name, type, and rule set. The ID is a randomly generated UUID, making it infeasible to guess. The name is opaque to Consul and human readable. The type is either "client" (meaning the token cannot modify ACL rules) or "management" (meaning the token is allowed to perform all actions).

The token ID is passed along with each RPC request to the servers. Consul's HTTP endpoints can accept tokens via the token query string parameter, or the X-Consul-Token request header.

By default, Consul will allow all actions.

Since Consul snapshots actually contain ACL tokens, the Snapshot API requires a management token for snapshot operations and does not use a special policy.

All nodes (clients and servers) must be configured with an acl_datacenter which enables ACL enforcement but also specifies the authoritative datacenter. Consul relies on RPC forwarding to support multi-datacenter configurations.

To avoid consistency issues, a single datacenter is considered authoritative and stores the canonical set of tokens.

When a request is made to an agent in a non-authoritative datacenter, it must be resolved into the appropriate policy. This is done by reading the token from the authoritative server and caching the result for a configurable acl_ttl. The implication of caching is that the cache TTL is an upper bound on the staleness of policy that is enforced. I

or the cache may be extended if the acl_down_policy is set accordingly.

Double check the ACL bootstrap guides!

-----------

Restores involve a potentially dangerous low-level Raft operation that is not designed to handle server failures during a restore. This command is primarily intended to be used when recovering from a disaster, restoring into a fresh cluster of Consul servers.
