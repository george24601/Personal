TiDB-Binlog is particularly useful when you migrate data from MySQL or MariaDB to TiDB, in which case you may use the TiDB DM (Data Migration) platform to get data from a MySQL/MariaDB cluster into TiDB, and then use TiDB-Binlog to keep a separate, downstream MySQL/MariaDB instance/cluster in sync with your TiDB cluster. TiDB-Binlog enables application traffic to TiDB to be pushed to a downstream MySQL or MariaDB instance/cluster, which reduces the risk of a migration to TiDB because you can easily revert the application to MySQL or MariaDB without downtime or data loss.

```
drainer.toml:
log-file="drainer.log"
[syncer]
db-type="mysql"
[syncer.to]
host="127.0.0.1"
user="root"
password=""
port=3306

pump.toml:
log-file="pump.log"
data-dir="pump.data"
addr="127.0.0.1:8250"
advertise-addr="127.0.0.1:8250"
pd-urls="http://127.0.0.1:2379"

tidb.toml:
store="tikv"
path="127.0.0.1:2379"
[log.file]
filename="tidb.log"
[binlog]
enable=true
```

Here we can already see the tidb_binlog database, which contains the checkpoint table used by drainer to record up to what point binary logs from the TiDB cluster have been applied.

Information about Pumps and Drainers that have joined the cluster is stored in PD.

If you kill a Drainer, the cluster puts it in the “paused” state, which means that the cluster expects it to rejoin,i.e., it is different from the "offline" mode! Use binglogctl to clean up the metadata


The main difference is that since TiDB is a distributed database, the binlog generated by each TiDB instance needs to be merged and sorted according to the time of the transaction commit before being consumed downstream.

 Drainer collects and merges the binlog from each Pump instance, and then converts the binlog into SQL statements or data in the specified format, and finally pushes the data to the downstream.

  A successful transaction writes two binlog records, including one Prewrite binlog record and one Commit binlog record. If the transaction fails, it will write a Rollback binlog record.

TiDB uses the built-in Pump Client to send the binlog to each Pump, pump needs SSD but no need to be super fast

  To use TiDB Binlog for recovering incremental data, set the config db-type to file (local files in the proto buffer format). Drainer converts the binlog to data in the specified proto buffer format and writes the data to local files. In this way, you can use Reparo to recover data incrementally.  

If the downstream is MySQL, MariaDB, or another TiDB cluster, you can use sync-diff-inspector to verify the data after data replication.


#I find error in pump replication, what can we do?
1. stop pumps
2. Full backup restore
3. restart pumps
 
The recommended startup sequence: PD -> TiKV -> Pump -> TiDB -> Drainer

# multiple downstream drainer?

# reparo

replicate-do-db specifies the database for recovery. If it is not set, all the databases are to be recovered.
replicate-do-table specifies the table for recovery. If it is not set, all the tables are to be recovered.

