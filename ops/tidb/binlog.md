```
drainer.toml:
log-file="drainer.log"
[syncer]
db-type="mysql"
[syncer.to]
host="127.0.0.1"
user="root"
password=""
port=3306

pump.toml:
log-file="pump.log"
data-dir="pump.data"
addr="127.0.0.1:8250"
advertise-addr="127.0.0.1:8250"
pd-urls="http://127.0.0.1:2379"

tidb.toml:
store="tikv"
path="127.0.0.1:2379"
[log.file]
filename="tidb.log"
[binlog]
enable=true
```

Here we can already see the tidb_binlog database, which contains the checkpoint table used by drainer to record up to what point binary logs from the TiDB cluster have been applied.

If you kill a Drainer, the cluster puts it in the “paused” state, which means that the cluster expects it to rejoin,i.e., it is different from the "offline" mode! Use binglogctl to clean up the metadata

The main difference is that since TiDB is a distributed database, the binlog generated by each TiDB instance needs to be merged and sorted according to the time of the transaction commit before being consumed downstream.

 Drainer collects and merges the binlog from each Pump instance, and then converts the binlog into SQL statements or data in the specified format, and finally pushes the data to the downstream.

  A successful transaction writes two binlog records, including one Prewrite binlog record and one Commit binlog record. If the transaction fails, it will write a Rollback binlog record.

  To use TiDB Binlog for recovering incremental data, set the config db-type to file (local files in the proto buffer format). Drainer converts the binlog to data in the specified proto buffer format and writes the data to local files. In this way, you can use Reparo to recover data incrementally.

If the downstream is MySQL, MariaDB, or another TiDB cluster, you can use sync-diff-inspector to verify the data after data replication.

### I find error in pump replication, what can we do?
1. stop pumps
2. Full backup restore
3. restart pumps
 
The recommended startup sequence: PD -> TiKV -> Pump -> TiDB -> Drainer

### drainer DR drill, what can we do?

### reparo

replicate-do-db specifies the database for recovery. If it is not set, all the databases are to be recovered.
replicate-do-table specifies the table for recovery. If it is not set, all the tables are to be recovered.

The data exported from MySQL contains a metadata file which includes the position information
The position information (Pos: 930143241) needs to be stored in the syncer.meta file for syncer to synchronize:
The syncer.meta file only needs to be configured once when it is first used. The position will be automatically updated when binlog is synchronized.

```syncer toml

log-level = "info"
log-file = "syncer.log"
log-rotate = "day"

server-id = 101

meta = "./syncer.meta"
worker-count = 16
batch = 1000
flavor = "mysql"

# The testing address for pprof. It can also be used by Prometheus to pull Syncer metrics.
status-addr = ":8271"

# If you set its value to true, Syncer stops and exits when it encounters the DDL operation.
stop-on-ddl = false

# max-retry is used for retry during network interruption.
max-retry = 100
```

TiDB-Binlog to replicate data from one TiDB cluster in one data center (DC) to another TiDB cluster in a different data center, both in the same city or close geographical proximity (e.g. an Availability Zone).

Pump records the binlog generated by TiDB in real time, and sorts the binlog according to the commit time of the transaction, and then provides it to Drainer for consumption.

Drainer collects and merges the binlog from each Pump instance, and then converts the binlog into SQL statements or data in the specified format, and finally pushes the data to the downstream.

The TiDB-Binlog format is row-based logging (RBL), which stores the change of each row of data. The data change is recorded in the prewrite_value field, and the data in this field mainly consists of serialized data arrays with the following TableMutation structure

Before replicating the binlog data to the downstream, Drainer needs to convert the above data back into SQL statements.

When writing binlog, TiDB sends a writing data request to TiKV and a Prewrite binlog record to Pump concurrently. If one of the two requests fails, the transaction will fail. After Prewrite succeeds, TiDB sends a commit message to TiKV , and then asynchronously sends a Commit binlog record to Pump. Since TiDB sends requests to TiKV and Pump in Prewrite phase at the same time, as long as the time that Pump handles the Prewrite binlog request is less than or equal to the time that TiKV executes Prewrite, enabling binlog will not affect the transaction delay.

by default pump keeps 7 days of data

make suare keep 1 pump on at alll times. 

To active drainer from existing cluster:
1. full backup and restore to get the savepoint
2. import the full backup
3. start the drainer from the endpoint

###mydumper output
1. metadata: start, end time, and binlog file postions
2. table data: every table has a file
3. table schemas
4. binary logs under binlog_snapshot

