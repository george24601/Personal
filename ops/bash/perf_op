#!/bin/bash
<<TOP
* Note that `top` has different layouts on centos and os x
* Are there too many users? Load average should be < 0.5 * # of cores
* If `us` is too high, then user processes are consuming much CPU resources, use `top -p $PID`
* If `sy` is too high, what is kernel working on?
* If `wa` is too high, the cpu idle waits for IO too much.
* If `Swap` > 0, are we running out of physical RAM?
* Check if `VIRT` is within your expectation - You should have a basic idea on how much virtual memory a process uses.

ni: high nice value user space process

Tasks
-------
PR: it is the process actual priority, as viewed by the Linux kernel, the scheduling priority of the process. For normal processes, the kernel priority is simply +20 from the nice value. Thus a process with the neutral nice value of zero has a kernel priority of 20. For processes running under real-time, the value of the field is RT

VIRT stands for the virtual size of a process, which is the sum of memory it is actually using, memory it has mapped into itself (for instance the video cardâ€™s RAM for the X server), files on disk that have been mapped into it (most notably shared libraries), and memory shared with other processes. VIRT represents how much memory the program is able to access at the present moment.  The  total  amount  of  virtual  memory  used  by the task.  It
      includes all code, data and shared libraries  plus  pages  that
      have been swapped out.

      VIRT = SWAP + RES.

RES: It is the resident memory size. Resident memory is the amount of non-swapped physical memory a task is using. It stands for the resident size, which is an accurate representation of how much actual physical memory a process is consuming. (This also corresponds directly to the %MEM column.)

SHR: SHR is the shared memory used by the process and it indicates how much of the VIRT size is actually sharable (memory or libraries). In the case of libraries, it does not necessarily mean that the entire library is resident.

%MEM: It represents the percentage of available physical memory used by the process.

TIME+: The total CPU time the task has used since it started, with precision up to hundredth of a second.

COMMAND: The command which was used to start the process

cache is page cache/disk cache in memory
cache for file system I/O, also used as other deviced's cache, e..g, block device
buffer cache is for block device IO -, also memory managed by block

number of online customers is more straightforward, number of concurent request is more accurate

For example, if a process memory-maps a large file, the file is actually stored on disk, but it still takes up "address space" in the process.

But even that isn't the whole answer. If a process calls fork(), it splits into two parts, and both of them initially share all their RSS. So even if RSS was initially 1 GB, the result after forking would be two processes, each with an RSS of 1 GB, but you'd still only be using 1 GB of memory.

Confused yet? Here's what you really need to know: use the free command and check the results before and after starting your program (on the +/- buffers/cache line). That difference is how much new memory your newly-started program used.

try to limit the average load to < 0.7 * CPU cores 
TOP


top -U franshesco

#sort by %CPU, %MEM to set mem
top -o %CPU

<<CS
note may need to install sysstat
check # of CSs, possible caues
1. normal CS by CPU
2. has higher priority tasks
3. I/O blocking
4. resourse contention and failed to acquire
CS
#check context switch times, 10 times, once per second, see if cs >> in. Can use pidstat on top of it
# if si(swap into memory) and so(swap out to harddrive) often > 0, means memory may not be enough
# us(user execution time) too high, means user process consumes too much CPU 
# wa(IO wait percentable), means IO wait too much
vmstat -1 -10

#grep oom_killer is /var/log/messages


#It is best to run iostat specifying a time interval in seconds (for example iostat -x 30)

#Display a continuous device report at two second intervals

<<IOSTAT
rrqm/s: The number of read requests merged per second that were queued to the device

rsec/s: The number of sectors read from the device per second.

avgrq-sz:The average size (in sectors) of the requests that were issued to the device.

await:The average time (in milliseconds) for I/O requests issued to the device to be served. This includes the time spent by the requests in queue and the time spent servicing them.

if await > svctm, often a sign of long IO queue

iowait in top: wait because of IO, CPU is idle at this time, not a strong indicator itself

%util
Percentage of CPU time during which I/O requests were issued to the device (bandwidth utilization for the device). Device saturation occurs when this value is close to 100%.

%idle < 70% often a sign of busy IO

IOSTAT
iostat x
iostat -dc 10

###jstack
###VM Thread often means the GC thread, nid is the OS thread id

#shows the heapsize
java -XX:+PrintFlagsFinal -version | grep HeapSize

#find jvm thread with highest cpu usage, can use jps to get java process id
#you can also use jps if you know the process name
top -H -p 63847

#find thread 2803, 2833 high cpu usage within that thread

# find java processes
ps axu | grep java

#thread dump
sudo jstack 63847 > 1.log

#need to covert decimal pid to hex
echo "obase=16; 3747" | bc

#if you need one-off stack, kill -3 would show the dump as well
kill  -3 63847

### info on GC 
jstat -gcxx -t pid $INTERVAL $COUNT
# if FGC > 5000, a sign of problem
jstate -gcutil 9 1000 10

#check heap usage 
jmap -heap -dump:file=xxx $PID 
#check object number and size
jmap -histo:live pid $PID 

#show java process id or can just ps aux | grep java. Here you can see the importance to giving meaninigful thread name
jps -v 

#view the port associated with a daemon
lsof -i -n -P | grep sendmail

#list open files in the open space
lsof -P -n | wc -l

#show deleted but still referenced files, so use 
lsof | grep deleted
#don't just rm! echo write instead. Otherwise, df still shows referenced file size
echo "" > tomcat.log

#check a port's connectivities
lsof -i:port

###last time  system online
uptime

###who are on this machine
who -a

###uname -a
system version

#under current dir, each dir's space
du -h --max-depth=1

#check a process's memory usage 
pmap $PID

# check process parent-child relationship
pstree 

### Note that 32G 128G physical machine can be considered high end already, should be able to handle 100k+ rqs

<<GC

reduce GC frequence to increasing heap size
reduce GC STW, by reducing heap space or use CMS
avoid Full gc by changing CMS trigger ration, give old gen more space, and incrase GC threads number

to debug: -XX:+PrintGCDtails -XX:+PrintGCDateStamps
GC

###fio
# slat: how long did it take to submit this IO to the kernel for processing?
# clat: time that passes between submission to the kernel and when the IO is complete, not including submission latency.
# Fio has an iodepth setting that controls how many IOs it issues to the OS at any given time.


# find process taking most CPU
ps H -eo pid,pcpu | sort -nk2 | tail
ll /proc/$PID

# check /var/log/messages for linux OOM killer action, i.e., "killed process"
# for check /var/log/hs_err_pid<pid>.log for jvm fatal erro
